{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armando5347/polaridad-opinion/blob/main/polaridad_de_opinion_limpiado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpwxKfk-2U7n"
      },
      "source": [
        "**Imports a utilizar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QJPUFQv_fqxr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import stanza\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import re, os\n",
        "import pickle\n",
        "import threading\n",
        "import gc\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import vstack\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWohy5iJthDN"
      },
      "source": [
        "**Normalizar el texto**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIAEtj_ZthDP"
      },
      "outputs": [],
      "source": [
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    reemplazos = {\n",
        "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
        "        'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
        "        'ä': 'a', 'ë': 'e', 'ï': 'i', 'ö': 'o', 'ü': 'u',\n",
        "        'ñ': 'n'\n",
        "    }\n",
        "\n",
        "    for acentuada, normal in reemplazos.items():\n",
        "        texto = texto.replace(acentuada, normal)\n",
        "\n",
        "    return texto\n",
        "\n",
        "config = {\n",
        "    'processors': 'tokenize,mwt,pos,lemma',\n",
        "    'lang': 'es'\n",
        "}\n",
        "\n",
        "nlp = stanza.Pipeline(**config)\n",
        "\n",
        "def normalizarTexto(texto, limpia, quita_stopwords, lematiza):\n",
        "    if limpia:\n",
        "        texto = limpiar_texto(texto)\n",
        "\n",
        "    try:\n",
        "        doc = nlp(texto)\n",
        "        cadenaNorm = \"\"\n",
        "        for sent in doc.sentences:\n",
        "            for token in sent.words:\n",
        "                if quita_stopwords and lematiza:\n",
        "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
        "                        cadenaNorm += token.lemma + \" \"\n",
        "                elif quita_stopwords:\n",
        "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
        "                        cadenaNorm += token.text + \" \"\n",
        "                elif lematiza:\n",
        "                    cadenaNorm += token.lemma + \" \"\n",
        "                else:\n",
        "                    cadenaNorm += token.text + \" \"\n",
        "    except:\n",
        "        cadenaNorm = \"\"\n",
        "\n",
        "    return cadenaNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ0cnRuDcnXs"
      },
      "source": [
        "**Normalización adicional**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQKxZAA-cnXv"
      },
      "outputs": [],
      "source": [
        "def corregir_texto(texto):\n",
        "\n",
        "    # Inicializa el corrector para el idioma español\n",
        "    spell = SpellChecker(language='es')\n",
        "\n",
        "    # Dividir el texto en palabras, preservando la puntuación\n",
        "    palabras = texto.split()\n",
        "    texto_corregido = []\n",
        "\n",
        "    for palabra in palabras:\n",
        "        # Extrae signos de puntuación al inicio y al final\n",
        "        inicio = ''.join(char for char in palabra if not char.isalnum())\n",
        "        final = ''.join(char for char in reversed(palabra) if not char.isalnum())\n",
        "        palabra_central = palabra[len(inicio):-len(final) or None]\n",
        "\n",
        "        # Si la palabra está mal escrita, corrige\n",
        "        if palabra_central and palabra_central not in spell:\n",
        "            sugerencia = spell.correction(palabra_central)\n",
        "            palabra_central = sugerencia if sugerencia else palabra_central\n",
        "\n",
        "        # Reconstruye la palabra con signos de puntuación\n",
        "        texto_corregido.append(f\"{inicio}{palabra_central}{final}\")\n",
        "\n",
        "    # Une las palabras corregidas en el texto final\n",
        "    return ' '.join(texto_corregido)\n",
        "\n",
        "\n",
        "\n",
        "def corregir_repeticiones(texto):\n",
        "\n",
        "    # Elimina repeticiones consecutivas de letras en una palabra (e.g., \"caaaarro\" -> \"carro\")\n",
        "    def corregir_letras_repetidas(palabra):\n",
        "        return re.sub(r'(.)\\1{2,}', r'\\1', palabra)\n",
        "\n",
        "    # Dividir el texto en palabras\n",
        "    palabras = texto.split()\n",
        "    palabras_corregidas = []\n",
        "    ultima_palabra = None\n",
        "\n",
        "    for palabra in palabras:\n",
        "        # Corregir letras repetidas en exceso\n",
        "        palabra_corregida = corregir_letras_repetidas(palabra)\n",
        "\n",
        "        # Eliminar palabras repetidas consecutivamente\n",
        "        if palabra_corregida != ultima_palabra:\n",
        "            palabras_corregidas.append(palabra_corregida)\n",
        "            ultima_palabra = palabra_corregida\n",
        "\n",
        "    # Reconstruir el texto corregido\n",
        "    return ' '.join(palabras_corregidas)\n",
        "\n",
        "\n",
        "def procesar_texto(texto, aplicar_repeticiones=False, aplicar_ortografia=False):\n",
        "\n",
        "    if aplicar_repeticiones:\n",
        "        texto = corregir_repeticiones(texto)\n",
        "    if aplicar_ortografia:\n",
        "        texto = corregir_texto(texto)\n",
        "    return texto\n",
        "def limpiado_adicional(texto):\n",
        "  return corregir_texto(corregir_repeticiones(texto))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx1a3JqU2k4v"
      },
      "source": [
        "**Obtener el dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dd51DDjO2n2x",
        "outputId": "ae86c132-5a39-4089-9ea4-980684dc29ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        1\n",
            "4        1\n",
            "        ..\n",
            "30207    5\n",
            "30208    5\n",
            "30209    5\n",
            "30210    5\n",
            "30211    5\n",
            "Name: Polarity, Length: 30212, dtype: UInt8\n"
          ]
        }
      ],
      "source": [
        "if (os.path.exists('./polaridad-opinion/corpusLimpiado.csv')):\n",
        "    data = pd.read_csv('./polaridad-opinion/corpusLimpiado.csv', sep='\\t', index = False)\n",
        "else:\n",
        "  data = pd.read_csv(\"./polaridad-opinion/corpusNorm.csv\", sep=\"\\t\")\n",
        "  #data['Content'] = data['Content'].apply(limpiado_adicional)\n",
        "  data.to_csv('./corpusLimpiado.csv', sep='\\t', index = False)\n",
        "data['Polarity'] = data['Polarity'].astype(\"UInt8\") #optimizando espacio\n",
        "train_split, test_split = train_test_split(\n",
        "        data,\n",
        "        test_size=0.2,  # 20% para prueba y 80% para entrenamiento\n",
        "        random_state=0,  # Semilla para asegurar reproducibilidad\n",
        "        stratify=data['Polarity']  # Mantener proporciones de clase\n",
        "    )\n",
        "\n",
        "X_train = train_split[\"Content\"]\n",
        "X_train_copy = X_train.copy()\n",
        "y_train = train_split[\"Polarity\"]\n",
        "y_train_copy = y_train.copy()\n",
        "X_test = test_split[\"Content\"]\n",
        "X_test_copy = X_test.copy()\n",
        "y_test = test_split[\"Polarity\"]\n",
        "y_test_copy = y_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBn7TlVthDW"
      },
      "source": [
        "**Calcular polaridad**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zkOjkZ2thDX"
      },
      "outputs": [],
      "source": [
        "def getSELFeatures(cadenas, lexicon_sel):\n",
        "\tpolaridad_cadenas = []\n",
        "\tfor cadena in cadenas:\n",
        "\t\tvalor_alegria = 0.0\n",
        "\t\tvalor_enojo = 0.0\n",
        "\t\tvalor_miedo = 0.0\n",
        "\t\tvalor_repulsion = 0.0\n",
        "\t\tvalor_sorpresa = 0.0\n",
        "\t\tvalor_tristeza = 0.0\n",
        "\t\tcadena_palabras = re.split('\\s+', cadena)\n",
        "\t\tdic = {}\n",
        "\t\tfor palabra in cadena_palabras:\n",
        "\t\t\tif palabra in lexicon_sel:\n",
        "\t\t\t\tcaracteristicas = lexicon_sel[palabra]\n",
        "\t\t\t\tfor emocion, valor in caracteristicas:\n",
        "\t\t\t\t\tif emocion == 'Alegría':\n",
        "\t\t\t\t\t\tvalor_alegria = valor_alegria + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Tristeza':\n",
        "\t\t\t\t\t\tvalor_tristeza = valor_tristeza + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Enojo':\n",
        "\t\t\t\t\t\tvalor_enojo = valor_enojo + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Repulsión':\n",
        "\t\t\t\t\t\tvalor_repulsion = valor_repulsion + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Miedo':\n",
        "\t\t\t\t\t\tvalor_miedo = valor_miedo + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Sorpresa':\n",
        "\t\t\t\t\t\tvalor_sorpresa = valor_sorpresa + float(valor)\n",
        "\t\tdic['__alegria__'] = valor_alegria\n",
        "\t\tdic['__tristeza__'] = valor_tristeza\n",
        "\t\tdic['__enojo__'] = valor_enojo\n",
        "\t\tdic['__repulsion__'] = valor_repulsion\n",
        "\t\tdic['__miedo__'] = valor_miedo\n",
        "\t\tdic['__sorpresa__'] = valor_sorpresa\n",
        "\n",
        "\t\t#Esto es para los valores acumulados del mapeo a positivo (alegría + sorpresa) y negativo (enojo + miedo + repulsión + tristeza)\n",
        "\t\tdic['acumuladopositivo'] = dic['__alegria__'] + dic['__sorpresa__']\n",
        "\t\tdic['acumuladonegative'] = dic['__enojo__'] + dic['__miedo__'] + dic['__repulsion__'] + dic['__tristeza__']\n",
        "\n",
        "\t\tpolaridad_pos = np.array([dic['acumuladopositivo']])\n",
        "\t\tpolaridad_neg = np.array([dic['acumuladonegative']])\n",
        "\t\tpolaridad_cadena = np.concatenate((polaridad_pos, polaridad_neg), axis=0)\n",
        "\t\tpolaridad_cadenas.append(polaridad_cadena)\n",
        "\t\tpolarida_cadenas = csr_matrix(polaridad_cadenas) #pasar a matriz dispersa, para reducir espacio\n",
        "\n",
        "\treturn polaridad_cadenas\n",
        "\n",
        "if (os.path.exists('./polaridad-opinion/lexicon_sel.pkl')):\n",
        "    lexicon_sel_file = open ('./polaridad-opinion/lexicon_sel.pkl','rb')\n",
        "    lexicon_sel = pickle.load(lexicon_sel_file)\n",
        "else:\n",
        "    print(\"No se ha encontrado el archivo lexicon_sel.pkl\")\n",
        "    exit()\n",
        "\n",
        "polaridad_train = getSELFeatures(X_train_copy, lexicon_sel)\n",
        "polaridad_test = getSELFeatures(X_test_copy, lexicon_sel)\n",
        "#print(polaridad_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMaW3IdBthDZ"
      },
      "source": [
        "**Vectorización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGYQRauSthDa"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_copy)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_copy)\n",
        "\n",
        "frequency_vectorizer = CountVectorizer(binary=False, ngram_range=(1, 1))\n",
        "X_train_freq = frequency_vectorizer.fit_transform(X_train_copy)\n",
        "X_test_freq = frequency_vectorizer.transform(X_test_copy)\n",
        "\n",
        "X_tain_pol = None\n",
        "X_test_pol = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54xnL6hMthDb"
      },
      "source": [
        "**Añadir polarización a la vectorización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETaf4IXVthDc",
        "outputId": "f549f2b8-78ea-4092-eee8-b8cb2665adb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_pol = hstack([X_train_tfidf, polaridad_train]).tocsr()\n",
        "X_test_pol = hstack([X_test_tfidf, polaridad_test]).tocsr()\n",
        "del X_train, X_train_copy, X_test, X_test_copy, X_test_tfidf\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmYMP0ufeX1"
      },
      "source": [
        "**Vectorización con SVD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuvffWQgfeX2"
      },
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=3000, random_state=0)\n",
        "X_train_tfidf_svd = csr_matrix(svd.fit_transform(X_train_tfidf))\n",
        "X_test_tfidf_svd = csr_matrix(svd.transform(X_test_tfidf))\n",
        "\n",
        "X_train_pol = hstack([X_train_tfidf_svd, polaridad_train]).tocsr()\n",
        "X_test_pol = hstack([X_test_tfidf_svd, polaridad_test]).tocsr()\n",
        "del X_train, X_train_copy, X_test, X_test_copy, X_test_tfidf, X_test_tfidf_svd, X_train_tfidf_svd\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA0zhwg5Eyys"
      },
      "source": [
        "**Prueba de balanceo de clases con cross validation con modelos no pesados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrzupjrcnX5"
      },
      "source": [
        "Utilizando polaridad de texto, undersampling y oversampling\n",
        "*Aplicando la función resample*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAbuDa2PE6r8"
      },
      "outputs": [],
      "source": [
        "pliegues = 5\n",
        "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
        "results = []\n",
        "modelo_prueba = SVC(random_state=0, C=10, kernel='linear', gamma=\"scale\")\n",
        "with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
        "  salida.write(\"Pruebas con MSV (C=10, kernel = 'linear', gamma='scale') con el corpus Normalizado\\nHaciendo pruebas de balanceo [3, 2.5, 2, 1.75, 0.5]\\n\\n\")\n",
        "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    X_trainn = X_train_pol[train_index]\n",
        "    X_testt = X_train_pol[test_index]\n",
        "    y_trainn = y_train_copy.iloc[train_index]\n",
        "    y_testt = y_train_copy.iloc[test_index]\n",
        "\n",
        "\n",
        "    class_counts = y_trainn.value_counts()\n",
        "\n",
        "    oversampling_factors = { #sujetos a cambio\n",
        "        1: 3,\n",
        "        2: 2.5,\n",
        "        3: 2,\n",
        "        4: 1.75,\n",
        "    }\n",
        "    undersampling_factor = 0.5\n",
        "\n",
        "    X_train_balanceado = []\n",
        "    y_train_balanceado = []\n",
        "\n",
        "    for polarity, count in class_counts.items():\n",
        "      class_data = X_trainn[y_trainn == polarity]\n",
        "      #y_class = y_train[y_train == polarity]\n",
        "\n",
        "      if polarity == 5:\n",
        "        target_size = int(count * undersampling_factor)\n",
        "        #class_data  = resample(class_data, replace=False, n_samples=target_size, random_state=0)\n",
        "        indices = resample(np.arange(X_trainn.shape[0]), replace=False, n_samples=target_size, random_state=0)\n",
        "      else:\n",
        "        factor = oversampling_factors.get(polarity, 1.0)\n",
        "        target_size = int(count * factor)\n",
        "        indices = resample(np.arange(X_trainn.shape[0]), replace=True, n_samples=target_size, random_state=0)\n",
        "        #class_data = resample(class_data, replace=True, n_samples=target_size, random_state=0)\n",
        "\n",
        "      X_balanceado_parcial = X_trainn[indices]\n",
        "      y_balanceado_parcial = y_trainn.iloc[indices]\n",
        "      X_train_balanceado.append(X_balanceado_parcial)\n",
        "      y_train_balanceado.append(y_balanceado_parcial)\n",
        "      #aqui acaba el for, que las identaciones se pusieron de comendiates\n",
        "\n",
        "    X_train_balanceado = vstack(X_train_balanceado)\n",
        "    y_train_balanceado = np.hstack(y_train_balanceado)\n",
        "    modelo_prueba.fit(X_train_balanceado, y_train_balanceado)\n",
        "\n",
        "    y_pred = modelo_prueba.predict(X_testt)\n",
        "    report = classification_report(y_testt, y_pred, output_dict=True, digits=4)\n",
        "    with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
        "      salida.write(str(report) + '\\n')\n",
        "    results.append(report['macro avg']['f1-score'])\n",
        "\n",
        "    #una vez termina todo lo de aquí, a limpiar\n",
        "    del X_trainn, X_testt, y_trainn, y_testt, X_train_balanceado\n",
        "    gc.collect()\n",
        "\n",
        "average_macro_f1 = sum(results) / len(results)\n",
        "printeado =f\"Promedio del f1-score de 'macro avg' en todas las iteraciones: {average_macro_f1}\"\n",
        "y_pred = modelo_prueba.predict(X_test_pol)\n",
        "with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
        "      salida.write(printeado + \"\\n\")\n",
        "      salida.write(str(classification_report(y_test, y_pred, output_dict=False, digits=4)))\n",
        "      salida.write(str(confusion_matrix(y_test, y_pred)) + \"\\n\")\n",
        "print(printeado)\n",
        "#print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X7eFrPrcnX6"
      },
      "source": [
        "Utilizando undersampling y oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKEa_HV6cnX7",
        "outputId": "8701fd80-d27e-4fc0-f0fd-722336d46a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promedio del f1-score de 'macro avg' en todas las iteraciones: 0.41603843745733526\n"
          ]
        }
      ],
      "source": [
        "pliegues = 5\n",
        "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
        "results = []\n",
        "modelo_prueba = LogisticRegression()\n",
        "\n",
        "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
        "\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    X_trainn, X_testt = X_train_pol[train_index], X_train_pol[test_index]\n",
        "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
        "\n",
        "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
        "    under_sampler = RandomUnderSampler(random_state=0)\n",
        "    over_sampling = RandomOverSampler(random_state=0)\n",
        "\n",
        "    # Paso 1: Under-sampling\n",
        "    X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
        "\n",
        "    # Paso 2: Over-sampling\n",
        "    X_resampled, y_resampled = over_sampling.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "    # Entrenar el modelo con los datos balanceados\n",
        "    modelo_prueba.fit(X_resampled, y_resampled)\n",
        "\n",
        "    y_pred = modelo_prueba.predict(X_testt)\n",
        "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
        "    results.append(report['macro avg']['f1-score'])\n",
        "average_macro_f1 = sum(results) / len(results)\n",
        "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n",
        "#print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_KRhWCs-GY1"
      },
      "source": [
        "**Crear pipeline, junto con los grid_search_view para los clasificadores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2gA3Lbv-OPT"
      },
      "outputs": [],
      "source": [
        "clasificadores = [SVC(random_state=0), MLPClassifier(max_iter=1000, random_state=0)]\n",
        "param_grid_svc = {\n",
        "                'classifier__C': [0.1, 1, 10],  # Hiperparámetro C para SVM\n",
        "                'classifier__kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n",
        "                'classifier__gamma': ['scale', 'auto']  # Parámetro gamma\n",
        "            }\n",
        "\n",
        "param_grid_mlp = {\n",
        "                'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "                'classifier__activation': ['tanh', 'relu'], #funcion de activacion\n",
        "                'classifier__alpha': [0.0001, 0.001, 0.01]\n",
        "            }\n",
        "def probarClasificador(clasificador, parametros, X_train, y_train, X_test, y_test, lock):\n",
        "\n",
        "\n",
        "\n",
        "  pipe = Pipeline([('text_representation', TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))), ('classifier',clasificador)])\n",
        "  #aqui, cv hace cross validation por su cuenta, y busca ajustar los mejores hipermarametros a partir del f1-macro\n",
        "  grid_search = GridSearchCV(pipe, parametros, cv=5,scoring='f1_macro')\n",
        "  if isinstance(clasificador, SVC):\n",
        "    print(\"Resultados de la maquina de soporte vectorial\")\n",
        "  else:\n",
        "    print(\"Resultados del perceptrón multicapa\")\n",
        "  # Entrenar el modelo con GridSearchCV\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_test)\n",
        "  with lock:\n",
        "    print(str(grid_search.best_params_))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "hilo_svc = threading.Thread(name=\"Experimento Maquina de soporte vectorial\",target=probarClasificador, args=(clasificadores[0], param_grid_svc, X_train, y_train, X_test, y_test, lock))\n",
        "hilo_mlp = threading.Thread(name=\"Experimento Perceptron multicapa\",target=probarClasificador, args=(clasificadores[1], param_grid_mlp, X_train_copy, y_train_copy, X_test_copy, y_test_copy, lock))\n",
        "#ejecutar hilos\n",
        "hilo_svc.start()\n",
        "hilo_mlp.start()\n",
        "#esperar a que terminen\n",
        "hilo_svc.join()\n",
        "hilo_mlp.join()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}