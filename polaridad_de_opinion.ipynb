{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armando5347/polaridad-opinion/blob/main/polaridad_de_opinion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpwxKfk-2U7n"
      },
      "source": [
        "**Imports a utilizar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NmnetZhNthDF",
        "outputId": "5ac89b20-6ff9-4116-e5e7-9f8e6322cbef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (4.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.6)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.14.0 stanza-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "QJPUFQv_fqxr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import stanza\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import re, os\n",
        "import pickle\n",
        "import threading\n",
        "from scipy.sparse import hstack\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWohy5iJthDN"
      },
      "source": [
        "**Normalizar el texto**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIAEtj_ZthDP"
      },
      "outputs": [],
      "source": [
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    reemplazos = {\n",
        "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
        "        'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
        "        'ä': 'a', 'ë': 'e', 'ï': 'i', 'ö': 'o', 'ü': 'u',\n",
        "        'ñ': 'n'\n",
        "    }\n",
        "\n",
        "    for acentuada, normal in reemplazos.items():\n",
        "        texto = texto.replace(acentuada, normal)\n",
        "\n",
        "    return texto\n",
        "\n",
        "config = {\n",
        "    'processors': 'tokenize,mwt,pos,lemma',\n",
        "    'lang': 'es'\n",
        "}\n",
        "\n",
        "nlp = stanza.Pipeline(**config)\n",
        "\n",
        "def normalizarTexto(texto, limpia, quita_stopwords, lematiza):\n",
        "    if limpia:\n",
        "        texto = limpiar_texto(texto)\n",
        "\n",
        "    try:\n",
        "        doc = nlp(texto)\n",
        "        cadenaNorm = \"\"\n",
        "        for sent in doc.sentences:\n",
        "            for token in sent.words:\n",
        "                if quita_stopwords and lematiza:\n",
        "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
        "                        cadenaNorm += token.lemma + \" \"\n",
        "                elif quita_stopwords:\n",
        "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
        "                        cadenaNorm += token.text + \" \"\n",
        "                elif lematiza:\n",
        "                    cadenaNorm += token.lemma + \" \"\n",
        "                else:\n",
        "                    cadenaNorm += token.text + \" \"\n",
        "    except:\n",
        "        cadenaNorm = \"\"\n",
        "\n",
        "    return cadenaNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx1a3JqU2k4v"
      },
      "source": [
        "**Obtener el dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dd51DDjO2n2x",
        "outputId": "c7e9257f-2615-4147-b5e4-d375b6820baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        pésimo lugar pensar dos vez antes ir hotel , m...\n",
            "1        no vayas lugar eddie cuatro ir recientemente E...\n",
            "2        mala relación calidad-precio seguir corto simp...\n",
            "3        minusválido ? ¡ no alojar aquí ! reservar hote...\n",
            "4        ser porqueria no perder tiempo no perder tiemp...\n",
            "                               ...                        \n",
            "30207    verdadera joya arquitectónico ser construcción...\n",
            "30208    romántico mucho estilo romeo julieta ser sitio...\n",
            "30209    parecer castillo ideal subir escalinata divisa...\n",
            "30210    imperdible ser imperdible , ahí poder ver much...\n",
            "30211    mucho bonito vista no poder ir guanajuato visi...\n",
            "Name: Content, Length: 30212, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"corpusNorm.csv\", sep=\"\\t\")\n",
        "print (data['Content'])\n",
        "\n",
        "train_split, test_split = train_test_split(\n",
        "        data,\n",
        "        test_size=0.2,  # 20% para prueba y 80% para entrenamiento\n",
        "        random_state=0,  # Semilla para asegurar reproducibilidad\n",
        "        stratify=data['Polarity']  # Mantener proporciones de clase\n",
        "    )\n",
        "\n",
        "X_train = train_split[\"Content\"]\n",
        "X_train_copy = X_train.copy()\n",
        "y_train = train_split[\"Polarity\"]\n",
        "y_train_copy = y_train.copy()\n",
        "X_test = test_split[\"Content\"]\n",
        "X_test_copy = X_test.copy()\n",
        "y_test = test_split[\"Polarity\"]\n",
        "y_test_copy = y_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBn7TlVthDW"
      },
      "source": [
        "**Calcular polaridad**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2zkOjkZ2thDX"
      },
      "outputs": [],
      "source": [
        "def getSELFeatures(cadenas, lexicon_sel):\n",
        "\t#'hastiar': [('Enojo\\n', '0.629'), ('Repulsi\\xf3n\\n', '0.596')]\n",
        "\tpolaridad_cadenas = []\n",
        "\tfor cadena in cadenas:\n",
        "\t\tvalor_alegria = 0.0\n",
        "\t\tvalor_enojo = 0.0\n",
        "\t\tvalor_miedo = 0.0\n",
        "\t\tvalor_repulsion = 0.0\n",
        "\t\tvalor_sorpresa = 0.0\n",
        "\t\tvalor_tristeza = 0.0\n",
        "\t\tcadena_palabras = re.split('\\s+', cadena)\n",
        "\t\tdic = {}\n",
        "\t\tfor palabra in cadena_palabras:\n",
        "\t\t\tif palabra in lexicon_sel:\n",
        "\t\t\t\tcaracteristicas = lexicon_sel[palabra]\n",
        "\t\t\t\tfor emocion, valor in caracteristicas:\n",
        "\t\t\t\t\tif emocion == 'Alegría':\n",
        "\t\t\t\t\t\tvalor_alegria = valor_alegria + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Tristeza':\n",
        "\t\t\t\t\t\tvalor_tristeza = valor_tristeza + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Enojo':\n",
        "\t\t\t\t\t\tvalor_enojo = valor_enojo + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Repulsión':\n",
        "\t\t\t\t\t\tvalor_repulsion = valor_repulsion + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Miedo':\n",
        "\t\t\t\t\t\tvalor_miedo = valor_miedo + float(valor)\n",
        "\t\t\t\t\telif emocion == 'Sorpresa':\n",
        "\t\t\t\t\t\tvalor_sorpresa = valor_sorpresa + float(valor)\n",
        "\t\tdic['__alegria__'] = valor_alegria\n",
        "\t\tdic['__tristeza__'] = valor_tristeza\n",
        "\t\tdic['__enojo__'] = valor_enojo\n",
        "\t\tdic['__repulsion__'] = valor_repulsion\n",
        "\t\tdic['__miedo__'] = valor_miedo\n",
        "\t\tdic['__sorpresa__'] = valor_sorpresa\n",
        "\n",
        "\t\t#Esto es para los valores acumulados del mapeo a positivo (alegría + sorpresa) y negativo (enojo + miedo + repulsión + tristeza)\n",
        "\t\tdic['acumuladopositivo'] = dic['__alegria__'] + dic['__sorpresa__']\n",
        "\t\tdic['acumuladonegative'] = dic['__enojo__'] + dic['__miedo__'] + dic['__repulsion__'] + dic['__tristeza__']\n",
        "\n",
        "\t\tpolaridad_pos = np.array([dic['acumuladopositivo']])\n",
        "\t\tpolaridad_neg = np.array([dic['acumuladonegative']])\n",
        "\t\tpolaridad_cadena = np.concatenate((polaridad_pos, polaridad_neg), axis=0)\n",
        "\t\tpolaridad_cadenas.append(polaridad_cadena)\n",
        "\n",
        "\treturn polaridad_cadenas\n",
        "\n",
        "if (os.path.exists('lexicon_sel.pkl')):\n",
        "    lexicon_sel_file = open ('lexicon_sel.pkl','rb')\n",
        "    lexicon_sel = pickle.load(lexicon_sel_file)\n",
        "else:\n",
        "    print(\"No se ha encontrado el archivo lexicon_sel.pkl\")\n",
        "    exit()\n",
        "\n",
        "polaridad_train = getSELFeatures(X_train, lexicon_sel)\n",
        "polaridad_test = getSELFeatures(X_test, lexicon_sel)\n",
        "# print(polaridad_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMaW3IdBthDZ"
      },
      "source": [
        "**Vectorización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SGYQRauSthDa"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_copy)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_copy)\n",
        "\n",
        "frequency_vectorizer = CountVectorizer(binary=False, ngram_range=(1, 1))\n",
        "X_train_freq = frequency_vectorizer.fit_transform(X_train_copy)\n",
        "X_test_freq = frequency_vectorizer.transform(X_test_copy)\n",
        "\n",
        "X_tain_pol = None\n",
        "X_test_pol = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54xnL6hMthDb"
      },
      "source": [
        "**Añadir polarización a la vectorización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ETaf4IXVthDc"
      },
      "outputs": [],
      "source": [
        "X_train_pol = hstack([X_train_tfidf, polaridad_train]).toarray()\n",
        "X_test_pol = hstack([X_test_tfidf, polaridad_test]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prueba de balanceo de clases con cross validation con modelos no pesados**"
      ],
      "metadata": {
        "id": "TA0zhwg5Eyys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pliegues = 5\n",
        "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
        "results = []\n",
        "modelo_prueba = LogisticRegression()\n",
        "\n",
        "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
        "\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    X_trainn, X_testt = X_train_pol[train_index], X_train_pol[test_index]\n",
        "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
        "\n",
        "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
        "    under_sampler = RandomUnderSampler(random_state=0)\n",
        "    over_sampling = RandomOverSampler(random_state=0)\n",
        "\n",
        "    # Paso 1: Under-sampling\n",
        "    X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
        "\n",
        "    # Paso 2: Over-sampling\n",
        "    X_resampled, y_resampled = over_sampling.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "    # Entrenar el modelo con los datos balanceados\n",
        "    modelo_prueba.fit(X_resampled, y_resampled)\n",
        "\n",
        "    y_pred = modelo_prueba.predict(X_testt)\n",
        "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
        "    results.append(report)\n",
        "average_results = pd.DataFrame(results).mean().to_dict()\n",
        "print(average_results)\n"
      ],
      "metadata": {
        "id": "oAbuDa2PE6r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_KRhWCs-GY1"
      },
      "source": [
        "**Crear pipeline, junto con los grid_search_view para los clasificadores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "H2gA3Lbv-OPT",
        "outputId": "c02e0d7c-4ca1-40d5-f336-880c0190c167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados de la maquina de soporte vectorial\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 103, in _analyze\n    doc = decoder(doc)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 236, in decode\n    raise ValueError(\nValueError: np.nan is an invalid document, expected byte or unicode string.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-40ff2f0d61e3>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resultados del perceptrón multicapa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Entrenar el modelo con GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 103, in _analyze\n    doc = decoder(doc)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 236, in decode\n    raise ValueError(\nValueError: np.nan is an invalid document, expected byte or unicode string.\n"
          ]
        }
      ],
      "source": [
        "clasificadores = [SVC(random_state=0), MLPClassifier(max_iter=1000, random_state=0)]\n",
        "param_grid_svc = {\n",
        "                'classifier__C': [0.1, 1, 10],  # Hiperparámetro C para SVM\n",
        "                'classifier__kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n",
        "                'classifier__gamma': ['scale', 'auto']  # Parámetro gamma\n",
        "            }\n",
        "\n",
        "param_grid_mlp = {\n",
        "                'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "                'classifier__activation': ['tanh', 'relu'], #funcion de activacion\n",
        "                'classifier__alpha': [0.0001, 0.001, 0.01]\n",
        "            }\n",
        "def probarClasificador(clasificador, parametros, X_train, y_train, X_test, y_test, lock):\n",
        "\n",
        "\n",
        "\n",
        "  pipe = Pipeline([('text_representation', TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))), ('classifier',clasificador)])\n",
        "  #aqui, cv hace cross validation por su cuenta, y busca ajustar los mejores hipermarametros a partir del f1-macro\n",
        "  grid_search = GridSearchCV(pipe, parametros, cv=5,scoring='f1_macro')\n",
        "  if isinstance(clasificador, SVC):\n",
        "    print(\"Resultados de la maquina de soporte vectorial\")\n",
        "  else:\n",
        "    print(\"Resultados del perceptrón multicapa\")\n",
        "  # Entrenar el modelo con GridSearchCV\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_test)\n",
        "  with lock:\n",
        "    print(str(grid_search.best_params_))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "lock = threading.Lock()\n",
        "\n",
        "hilo_svc = threading.Thread(name=\"Experimento Maquina de soporte vectorial\",target=probarClasificador, args=(clasificadores[0], param_grid_svc, X_train, y_train, X_test, y_test, lock))\n",
        "hilo_mlp = threading.Thread(name=\"Experimento Perceptron multicapa\",target=probarClasificador, args=(clasificadores[1], param_grid_mlp, X_train_copy, y_train_copy, X_test_copy, y_test_copy, lock))\n",
        "#ejecutar hilos\n",
        "hilo_svc.start()\n",
        "hilo_mlp.start()\n",
        "#esperar a que terminen\n",
        "hilo_svc.join()\n",
        "hilo_mlp.join()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}