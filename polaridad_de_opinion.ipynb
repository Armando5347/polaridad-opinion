{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Armando5347/polaridad-opinion/blob/main/polaridad_de_opinion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGiaJ1uxdg8Y"
   },
   "source": [
    "\n",
    "**Comandos previos a la ejecución del código**\n",
    "\n",
    "Incluye conar el repositorio e instalar las librerias presentes en los requerimientos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "vLwRVtlEcruw",
    "outputId": "a9abf572-46c8-4bc6-8a33-b848b9b3283d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'polaridad-opinion'...\n",
      "remote: Enumerating objects: 59, done.\u001b[K\n",
      "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
      "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
      "remote: Total 59 (delta 29), reused 21 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (59/59), 4.78 MiB | 12.04 MiB/s, done.\n",
      "Resolving deltas: 100% (29/29), done.\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r ./polaridad-opinion/requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r ./polaridad-opinion/requirements.txt (line 2)) (1.5.2)\n",
      "Collecting imblearn (from -r ./polaridad-opinion/requirements.txt (line 3))\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting stanza (from -r ./polaridad-opinion/requirements.txt (line 4))\n",
      "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pyspellchecker (from -r ./polaridad-opinion/requirements.txt (line 5))\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->-r ./polaridad-opinion/requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r ./polaridad-opinion/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r ./polaridad-opinion/requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r ./polaridad-opinion/requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r ./polaridad-opinion/requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r ./polaridad-opinion/requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r ./polaridad-opinion/requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn->-r ./polaridad-opinion/requirements.txt (line 3)) (0.12.4)\n",
      "Collecting emoji (from stanza->-r ./polaridad-opinion/requirements.txt (line 4))\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (4.25.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (4.66.6)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r ./polaridad-opinion/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza->-r ./polaridad-opinion/requirements.txt (line 4)) (3.0.2)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker, emoji, stanza, imblearn\n",
      "Successfully installed emoji-2.14.0 imblearn-0.0 pyspellchecker-0.8.1 stanza-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Armando5347/polaridad-opinion.git\n",
    "!pip install -r ./polaridad-opinion/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpwxKfk-2U7n"
   },
   "source": [
    "**Imports a utilizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QJPUFQv_fqxr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re, os\n",
    "import pickle\n",
    "import threading\n",
    "import gc\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWohy5iJthDN"
   },
   "source": [
    "**Normalizar el texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bb7bef03f9a84363bbb570990fbaa529"
     ]
    },
    "id": "aIAEtj_ZthDP",
    "outputId": "f39999fe-94a7-40b8-9d45-1703116202e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 11:43:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7bef03f9a84363bbb570990fbaa529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 11:43:29 INFO: Downloaded file to C:\\Users\\emidh\\stanza_resources\\resources.json\n",
      "2024-11-23 11:43:29 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-23 11:43:29 INFO: Using device: cuda\n",
      "2024-11-23 11:43:29 INFO: Loading: tokenize\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texto\n\u001b[0;32m     15\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessors\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize,mwt,pos,lemma\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[1;32m---> 20\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalizarTexto\u001b[39m(texto, limpia, quita_stopwords, lematiza):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m limpia:\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\pipeline\\core.py:308\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(curr_processor_config)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name] \u001b[38;5;241m=\u001b[39m \u001b[43mNAME_TO_PROCESSOR_CLASS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_processor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessorRequirementsException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;66;03m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[0;32m    313\u001b[0m     pipeline_reqs_exceptions\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\pipeline\\processor.py:193\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[1;34m(self, config, pipeline, device)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_variant\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# build the final config for the processor\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\pipeline\\tokenize_processor.py:44\u001b[0m, in \u001b[0;36mTokenizeProcessor._set_up_model\u001b[1;34m(self, config, pipeline, device)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# get and typecheck the postprocessor\u001b[39;00m\n\u001b[0;32m     47\u001b[0m postprocessor \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:30\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, args, vocab, lexicon, dictionary, model_file, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_funcs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\models\\common\\utils.py:263\u001b[0m, in \u001b[0;36mget_optimizer\u001b[1;34m(name, model, lr, betas, eps, momentum, weight_decay, bert_learning_rate, bert_weight_decay, charlm_learning_rate, is_peft, bert_finetune_layers, opt_logger)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     extra_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weight_decay\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stanza\\models\\common\\utils.py:186\u001b[0m, in \u001b[0;36mdispatch_optimizer\u001b[1;34m(name, parameters, opt_logger, lr, betas, eps, momentum, **extra_args)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    185\u001b[0m     opt_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding Adam with lr=\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, betas=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, eps=\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lr, betas, eps, extra_logging)\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madamw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    188\u001b[0m     opt_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding AdamW with lr=\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, betas=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, eps=\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lr, betas, eps, extra_logging)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compiling\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashable, is_safe_constant, NP_SUPPORTED_MODULES\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mA note on allowed functions:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mskipfiles\" there.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFunctionIdSet\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\utils.py:89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeGuard, Source, TracingContext\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloorDiv, Mod, IsNonOverlappingAndDenseIndicator\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_solve\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalue_ranges\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bound_sympy, SymPyValueRangeAnalysis, ValueRanges, ValueRangeError\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzzy_and, fuzzy_not, fuzzy_or\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\__init__.py:74\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[0;32m     68\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[0;32m     69\u001b[0m         true, false, satisfiable)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[0;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[0;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[0;32m     77\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[0;32m     78\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[0;32m     79\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[0;32m     80\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, real_roots,\n\u001b[0;32m     81\u001b[0m         nroots, ground_roots, nth_power_roots_poly, cancel, reduced, groebner,\n\u001b[0;32m     82\u001b[0m         is_zero_dimensional, GroebnerBasis, poly, symmetrize, horner,\n\u001b[0;32m     83\u001b[0m         interpolate, rational_interpolate, viete, together,\n\u001b[0;32m     84\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[0;32m     85\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[0;32m     86\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[0;32m     87\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[0;32m     88\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[0;32m     89\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[0;32m     90\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[0;32m     91\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[0;32m     92\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[0;32m     93\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[0;32m     94\u001b[0m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[0;32m     95\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[0;32m     96\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[0;32m     97\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[0;32m     98\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[0;32m     99\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[0;32m    100\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[0;32m    101\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[0;32m    102\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[0;32m    103\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[0;32m    104\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[0;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[0;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[0;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[0;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[0;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[0;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[0;32m    134\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\__init__.py:68\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Polynomial manipulation algorithms and algebraic objects. \"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPurePoly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly_from_expr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_poly_from_expr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_degree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree_list\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpquo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfield\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m ]\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolytools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr,\n\u001b[0;32m     69\u001b[0m         parallel_poly_from_expr, degree, total_degree, degree_list, LC, LM,\n\u001b[0;32m     70\u001b[0m         LT, pdiv, prem, pquo, pexquo, div, rem, quo, exquo, half_gcdex, gcdex,\n\u001b[0;32m     71\u001b[0m         invert, subresultants, resultant, discriminant, cofactors, gcd_list,\n\u001b[0;32m     72\u001b[0m         gcd, lcm_list, lcm, terms_gcd, trunc, monic, content, primitive,\n\u001b[0;32m     73\u001b[0m         compose, decompose, sturm, gff_list, gff, sqf_norm, sqf_part,\n\u001b[0;32m     74\u001b[0m         sqf_list, sqf, factor_list, factor, intervals, refine_root,\n\u001b[0;32m     75\u001b[0m         count_roots, real_roots, nroots, ground_roots, nth_power_roots_poly,\n\u001b[0;32m     76\u001b[0m         cancel, reduced, groebner, is_zero_dimensional, GroebnerBasis, poly)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfuncs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (symmetrize, horner, interpolate,\n\u001b[0;32m     79\u001b[0m         rational_interpolate, viete)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrationaltools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m together\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\polytools.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanAtom\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polyoptions \u001b[38;5;28;01mas\u001b[39;00m options\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstructor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m construct_domain\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FF, QQ, ZZ\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomainelement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DomainElement\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\constructor.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevalf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pure_complex\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ordered\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZZ, QQ, ZZ_I, QQ_I, EX\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplexfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexField\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrealfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RealField\n",
      "File \u001b[1;32mc:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\domains\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfractionfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FractionField\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpressiondomain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpressionDomain, EX\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpressionrawdomain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EXRAW\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpythonrational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonRational\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# This is imported purely for backwards compatibility because some parts of\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# the codebase used to import this from here and it's possible that downstream\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# does as well:\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    reemplazos = {\n",
    "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "        'à': 'a', 'è': 'e', 'ì': 'i', 'ò': 'o', 'ù': 'u',\n",
    "        'ä': 'a', 'ë': 'e', 'ï': 'i', 'ö': 'o', 'ü': 'u',\n",
    "        'ñ': 'n'\n",
    "    }\n",
    "\n",
    "    for acentuada, normal in reemplazos.items():\n",
    "        texto = texto.replace(acentuada, normal)\n",
    "\n",
    "    return texto\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,mwt,pos,lemma',\n",
    "    'lang': 'es'\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config)\n",
    "\n",
    "def normalizarTexto(texto, limpia, quita_stopwords, lematiza):\n",
    "    if limpia:\n",
    "        texto = limpiar_texto(texto)\n",
    "\n",
    "    try:\n",
    "        doc = nlp(texto)\n",
    "        cadenaNorm = \"\"\n",
    "        for sent in doc.sentences:\n",
    "            for token in sent.words:\n",
    "                if quita_stopwords and lematiza:\n",
    "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
    "                        cadenaNorm += token.lemma + \" \"\n",
    "                elif quita_stopwords:\n",
    "                    if token.pos not in {'ADP', 'CCONJ', 'DET', 'SCONJ', 'PRON'}:\n",
    "                        cadenaNorm += token.text + \" \"\n",
    "                elif lematiza:\n",
    "                    cadenaNorm += token.lemma + \" \"\n",
    "                else:\n",
    "                    cadenaNorm += token.text + \" \"\n",
    "    except:\n",
    "        cadenaNorm = \"\"\n",
    "\n",
    "    return cadenaNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ0cnRuDcnXs"
   },
   "source": [
    "**Normalización adicional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQKxZAA-cnXv"
   },
   "outputs": [],
   "source": [
    "def corregir_texto(texto):\n",
    "\n",
    "    # Inicializa el corrector para el idioma español\n",
    "    spell = SpellChecker(language='es')\n",
    "\n",
    "    # Dividir el texto en palabras, preservando la puntuación\n",
    "    palabras = texto.split()\n",
    "    texto_corregido = []\n",
    "\n",
    "    for palabra in palabras:\n",
    "        # Extrae signos de puntuación al inicio y al final\n",
    "        inicio = ''.join(char for char in palabra if not char.isalnum())\n",
    "        final = ''.join(char for char in reversed(palabra) if not char.isalnum())\n",
    "        palabra_central = palabra[len(inicio):-len(final) or None]\n",
    "\n",
    "        # Si la palabra está mal escrita, corrige\n",
    "        if palabra_central and palabra_central not in spell:\n",
    "            sugerencia = spell.correction(palabra_central)\n",
    "            palabra_central = sugerencia if sugerencia else palabra_central\n",
    "\n",
    "        # Reconstruye la palabra con signos de puntuación\n",
    "        texto_corregido.append(f\"{inicio}{palabra_central}{final}\")\n",
    "\n",
    "    # Une las palabras corregidas en el texto final\n",
    "    return ' '.join(texto_corregido)\n",
    "\n",
    "\n",
    "\n",
    "def corregir_repeticiones(texto):\n",
    "\n",
    "    # Elimina repeticiones consecutivas de letras en una palabra (e.g., \"caaaarro\" -> \"carro\")\n",
    "    def corregir_letras_repetidas(palabra):\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1', palabra)\n",
    "\n",
    "    # Dividir el texto en palabras\n",
    "    palabras = texto.split()\n",
    "    palabras_corregidas = []\n",
    "    ultima_palabra = None\n",
    "\n",
    "    for palabra in palabras:\n",
    "        # Corregir letras repetidas en exceso\n",
    "        palabra_corregida = corregir_letras_repetidas(palabra)\n",
    "\n",
    "        # Eliminar palabras repetidas consecutivamente\n",
    "        if palabra_corregida != ultima_palabra:\n",
    "            palabras_corregidas.append(palabra_corregida)\n",
    "            ultima_palabra = palabra_corregida\n",
    "\n",
    "    # Reconstruir el texto corregido\n",
    "    return ' '.join(palabras_corregidas)\n",
    "\n",
    "\n",
    "def procesar_texto(texto, aplicar_repeticiones=False, aplicar_ortografia=False):\n",
    "\n",
    "    if aplicar_repeticiones:\n",
    "        texto = corregir_repeticiones(texto)\n",
    "    if aplicar_ortografia:\n",
    "        texto = corregir_texto(texto)\n",
    "    return texto\n",
    "def limpiado_adicional(texto):\n",
    "  return corregir_texto(corregir_repeticiones(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx1a3JqU2k4v"
   },
   "source": [
    "**Obtener el dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Dd51DDjO2n2x",
    "outputId": "ae86c132-5a39-4089-9ea4-980684dc29ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "30207    5\n",
      "30208    5\n",
      "30209    5\n",
      "30210    5\n",
      "30211    5\n",
      "Name: Polarity, Length: 30212, dtype: UInt8\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists('./polaridad-opinion/corpusLimpiado.csv')):\n",
    "    data = pd.read_csv('./polaridad-opinion/corpusLimpiado.csv', sep='\\t', index = False)\n",
    "else:\n",
    "  data = pd.read_csv(\"./polaridad-opinion/corpusNorm.csv\", sep=\"\\t\")\n",
    "  #data['Content'] = data['Content'].apply(limpiado_adicional)\n",
    "  data.to_csv('./corpusLimpiado.csv', sep='\\t', index = False)\n",
    "data['Polarity'] = data['Polarity'].astype(\"UInt8\") #optimizando espacio, xd\n",
    "print (data['Polarity'])\n",
    "\n",
    "train_split, test_split = train_test_split(\n",
    "        data,\n",
    "        test_size=0.2,  # 20% para prueba y 80% para entrenamiento\n",
    "        random_state=0,  # Semilla para asegurar reproducibilidad\n",
    "        stratify=data['Polarity']  # Mantener proporciones de clase\n",
    "    )\n",
    "\n",
    "X_train = train_split[\"Content\"]\n",
    "X_train_copy = X_train.copy()\n",
    "y_train = train_split[\"Polarity\"]\n",
    "y_train_copy = y_train.copy()\n",
    "X_test = test_split[\"Content\"]\n",
    "X_test_copy = X_test.copy()\n",
    "y_test = test_split[\"Polarity\"]\n",
    "y_test_copy = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yBn7TlVthDW"
   },
   "source": [
    "**Calcular polaridad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2zkOjkZ2thDX"
   },
   "outputs": [],
   "source": [
    "def getSELFeatures(cadenas, lexicon_sel):\n",
    "\t#'hastiar': [('Enojo\\n', '0.629'), ('Repulsi\\xf3n\\n', '0.596')]\n",
    "\tpolaridad_cadenas = []\n",
    "\tfor cadena in cadenas:\n",
    "\t\tvalor_alegria = 0.0\n",
    "\t\tvalor_enojo = 0.0\n",
    "\t\tvalor_miedo = 0.0\n",
    "\t\tvalor_repulsion = 0.0\n",
    "\t\tvalor_sorpresa = 0.0\n",
    "\t\tvalor_tristeza = 0.0\n",
    "\t\tcadena_palabras = re.split('\\s+', cadena)\n",
    "\t\tdic = {}\n",
    "\t\tfor palabra in cadena_palabras:\n",
    "\t\t\tif palabra in lexicon_sel:\n",
    "\t\t\t\tcaracteristicas = lexicon_sel[palabra]\n",
    "\t\t\t\tfor emocion, valor in caracteristicas:\n",
    "\t\t\t\t\tif emocion == 'Alegría':\n",
    "\t\t\t\t\t\tvalor_alegria = valor_alegria + float(valor)\n",
    "\t\t\t\t\telif emocion == 'Tristeza':\n",
    "\t\t\t\t\t\tvalor_tristeza = valor_tristeza + float(valor)\n",
    "\t\t\t\t\telif emocion == 'Enojo':\n",
    "\t\t\t\t\t\tvalor_enojo = valor_enojo + float(valor)\n",
    "\t\t\t\t\telif emocion == 'Repulsión':\n",
    "\t\t\t\t\t\tvalor_repulsion = valor_repulsion + float(valor)\n",
    "\t\t\t\t\telif emocion == 'Miedo':\n",
    "\t\t\t\t\t\tvalor_miedo = valor_miedo + float(valor)\n",
    "\t\t\t\t\telif emocion == 'Sorpresa':\n",
    "\t\t\t\t\t\tvalor_sorpresa = valor_sorpresa + float(valor)\n",
    "\t\tdic['__alegria__'] = valor_alegria\n",
    "\t\tdic['__tristeza__'] = valor_tristeza\n",
    "\t\tdic['__enojo__'] = valor_enojo\n",
    "\t\tdic['__repulsion__'] = valor_repulsion\n",
    "\t\tdic['__miedo__'] = valor_miedo\n",
    "\t\tdic['__sorpresa__'] = valor_sorpresa\n",
    "\n",
    "\t\t#Esto es para los valores acumulados del mapeo a positivo (alegría + sorpresa) y negativo (enojo + miedo + repulsión + tristeza)\n",
    "\t\tdic['acumuladopositivo'] = dic['__alegria__'] + dic['__sorpresa__']\n",
    "\t\tdic['acumuladonegative'] = dic['__enojo__'] + dic['__miedo__'] + dic['__repulsion__'] + dic['__tristeza__']\n",
    "\n",
    "\t\tpolaridad_pos = np.array([dic['acumuladopositivo']])\n",
    "\t\tpolaridad_neg = np.array([dic['acumuladonegative']])\n",
    "\t\tpolaridad_cadena = np.concatenate((polaridad_pos, polaridad_neg), axis=0)\n",
    "\t\tpolaridad_cadenas.append(polaridad_cadena)\n",
    "\t\tpolarida_cadenas = csr_matrix(polaridad_cadenas) #pasar a matriz dispersa, para reducir espacio\n",
    "\n",
    "\treturn polaridad_cadenas\n",
    "\n",
    "if (os.path.exists('./polaridad-opinion/lexicon_sel.pkl')):\n",
    "    lexicon_sel_file = open ('./polaridad-opinion/lexicon_sel.pkl','rb')\n",
    "    lexicon_sel = pickle.load(lexicon_sel_file)\n",
    "else:\n",
    "    print(\"No se ha encontrado el archivo lexicon_sel.pkl\")\n",
    "    exit()\n",
    "\n",
    "polaridad_train = getSELFeatures(X_train_copy, lexicon_sel)\n",
    "polaridad_test = getSELFeatures(X_test_copy, lexicon_sel)\n",
    "#print(polaridad_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMaW3IdBthDZ"
   },
   "source": [
    "**Vectorización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SGYQRauSthDa"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_copy)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_copy)\n",
    "\n",
    "frequency_vectorizer = CountVectorizer(binary=False, ngram_range=(1, 1))\n",
    "X_train_freq = frequency_vectorizer.fit_transform(X_train_copy)\n",
    "X_test_freq = frequency_vectorizer.transform(X_test_copy)\n",
    "\n",
    "X_tain_pol = None\n",
    "X_test_pol = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54xnL6hMthDb"
   },
   "source": [
    "**Añadir polarización a la vectorización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETaf4IXVthDc",
    "outputId": "f549f2b8-78ea-4092-eee8-b8cb2665adb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pol = hstack([X_train_tfidf, polaridad_train]).tocsr()\n",
    "X_test_pol = hstack([X_test_tfidf, polaridad_test]).tocsr()\n",
    "del X_train, X_train_copy, X_test, X_test_copy, X_test_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorización con SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3000, random_state=0)\n",
    "X_train_tfidf_svd = csr_matrix(svd.fit_transform(X_train_tfidf))\n",
    "X_test_tfidf_svd = csr_matrix(svd.transform(X_test_tfidf))\n",
    "\n",
    "X_train_pol = hstack([X_train_tfidf_svd, polaridad_train]).tocsr()\n",
    "X_test_pol = hstack([X_test_tfidf_svd, polaridad_test]).tocsr()\n",
    "del X_train, X_train_copy, X_test, X_test_copy, X_test_tfidf, X_test_tfidf_svd, X_train_tfidf_svd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA0zhwg5Eyys"
   },
   "source": [
    "**Prueba de balanceo de clases con cross validation con modelos no pesados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZrzupjrcnX5"
   },
   "source": [
    "Utilizando polaridad de texto, undersampling y oversampling\n",
    "*Aplicando al forma en la que **Bambino** lo haría*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAbuDa2PE6r8"
   },
   "outputs": [],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_prueba = SVC(random_state=0, C=10, kernel='linear', gamma=\"scale\")\n",
    "with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
    "  salida.write(\"Pruebas con MSV (C=10, kernel = 'linear', gamma='scale') con el corpus Normalizado\\nHaciendo pruebas de balanceo [3, 2.5, 2, 1.75, 0.5]\\n\\n\")\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn = X_train_pol[train_index]\n",
    "    X_testt = X_train_pol[test_index]\n",
    "    y_trainn = y_train_copy.iloc[train_index]\n",
    "    y_testt = y_train_copy.iloc[test_index]\n",
    "\n",
    "\n",
    "    class_counts = y_trainn.value_counts()\n",
    "\n",
    "    oversampling_factors = { #sujetos a cambio\n",
    "        1: 3,\n",
    "        2: 2.5,\n",
    "        3: 2,\n",
    "        4: 1.75,\n",
    "    }\n",
    "    undersampling_factor = 0.5\n",
    "\n",
    "    X_train_balanceado = []\n",
    "    y_train_balanceado = []\n",
    "\n",
    "    for polarity, count in class_counts.items():\n",
    "      class_data = X_trainn[y_trainn == polarity]\n",
    "      #y_class = y_train[y_train == polarity]\n",
    "\n",
    "      if polarity == 5:\n",
    "        target_size = int(count * undersampling_factor)\n",
    "        #class_data  = resample(class_data, replace=False, n_samples=target_size, random_state=0)\n",
    "        indices = resample(np.arange(X_trainn.shape[0]), replace=False, n_samples=target_size, random_state=0)\n",
    "      else:\n",
    "        factor = oversampling_factors.get(polarity, 1.0)\n",
    "        target_size = int(count * factor)\n",
    "        indices = resample(np.arange(X_trainn.shape[0]), replace=True, n_samples=target_size, random_state=0)\n",
    "        #class_data = resample(class_data, replace=True, n_samples=target_size, random_state=0)\n",
    "\n",
    "      X_balanceado_parcial = X_trainn[indices]\n",
    "      y_balanceado_parcial = y_trainn.iloc[indices]\n",
    "      X_train_balanceado.append(X_balanceado_parcial)\n",
    "      y_train_balanceado.append(y_balanceado_parcial)\n",
    "      #aqui acaba el for, que las identaciones se pusieron de comendiates\n",
    "\n",
    "    X_train_balanceado = vstack(X_train_balanceado)\n",
    "    y_train_balanceado = np.hstack(y_train_balanceado)\n",
    "    modelo_prueba.fit(X_train_balanceado, y_train_balanceado)\n",
    "\n",
    "    y_pred = modelo_prueba.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True, digits=4)\n",
    "    with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
    "      salida.write(str(report) + '\\n')\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "\n",
    "    #una vez termina todo lo de aquí, a limpiar\n",
    "    del X_trainn, X_testt, y_trainn, y_testt, X_train_balanceado\n",
    "    gc.collect()\n",
    "\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "printeado =f\"Promedio del f1-score de 'macro avg' en todas las iteraciones: {average_macro_f1}\"\n",
    "y_pred = modelo_prueba.predict(X_test_pol)\n",
    "with open('./resultados_pruebas.txt','a', encoding='utf-8') as salida:\n",
    "      salida.write(printeado + \"\\n\")\n",
    "      salida.write(str(classification_report(y_test, y_pred, output_dict=False, digits=4)))\n",
    "      salida.write(str(confusion_matrix(y_test, y_pred)) + \"\\n\")\n",
    "print(printeado)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X7eFrPrcnX6"
   },
   "source": [
    "Utilizando undersampling y oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKEa_HV6cnX7",
    "outputId": "8701fd80-d27e-4fc0-f0fd-722336d46a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio del f1-score de 'macro avg' en todas las iteraciones: 0.41603843745733526\n"
     ]
    }
   ],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_prueba = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
    "\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn, X_testt = X_train_pol[train_index], X_train_pol[test_index]\n",
    "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
    "\n",
    "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
    "    under_sampler = RandomUnderSampler(random_state=0)\n",
    "    over_sampling = RandomOverSampler(random_state=0)\n",
    "\n",
    "    # Paso 1: Under-sampling\n",
    "    X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Paso 2: Over-sampling\n",
    "    X_resampled, y_resampled = over_sampling.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    # Entrenar el modelo con los datos balanceados\n",
    "    modelo_prueba.fit(X_resampled, y_resampled)\n",
    "\n",
    "    y_pred = modelo_prueba.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhzL-nlPcnX7"
   },
   "source": [
    "Utilizando solo undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjqRkHGmcnX8",
    "outputId": "0252f59a-9e00-489f-d0a1-c68543de3d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio del f1-score de 'macro avg' en todas las iteraciones: 0.41664715122745777\n"
     ]
    }
   ],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_prueba = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_tfidf, y_train_copy):\n",
    "\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn, X_testt = X_train_tfidf[train_index], X_train_tfidf[test_index]\n",
    "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
    "\n",
    "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
    "    under_sampler = RandomUnderSampler(random_state=0)\n",
    "    over_sampling = RandomOverSampler(random_state=0)\n",
    "\n",
    "    # Paso 1: Under-sampling\n",
    "    X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Paso 2: Over-sampling\n",
    "    #X_resampled, y_resampled = over_sampling.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    # Entrenar el modelo con los datos balanceados\n",
    "    modelo_prueba.fit(X_resampled, y_resampled)\n",
    "\n",
    "    y_pred = modelo_prueba.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_2pnzfTcnX8"
   },
   "source": [
    "Utilizando solo oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3o2ISmKcnX9",
    "outputId": "880dbeda-1f53-438c-9f56-c25e3dc5791e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio del f1-score de 'macro avg' en todas las iteraciones: 0.4759675912656955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emidh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_prueba = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_tfidf, y_train_copy):\n",
    "\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn, X_testt = X_train_tfidf[train_index], X_train_tfidf[test_index]\n",
    "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
    "\n",
    "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
    "    under_sampler = RandomUnderSampler(random_state=0)\n",
    "    over_sampling = RandomOverSampler(random_state=0)\n",
    "\n",
    "    # Paso 1: Under-sampling\n",
    "    #X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Paso 2: Over-sampling\n",
    "    X_resampled, y_resampled = over_sampling.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Entrenar el modelo con los datos balanceados\n",
    "    modelo_prueba.fit(X_resampled, y_resampled)\n",
    "\n",
    "    y_pred = modelo_prueba.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVa1rVoNcnX9"
   },
   "source": [
    "Prueba con el módelo pesado, en ese caso, la maquina de soporte vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "2Cn1f62rcnX-",
    "outputId": "c771c3f8-2835-414b-c27f-859eda200d38"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8bec040eb4db>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mX_train_balanceado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanceado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0my_train_balanceado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_balanceado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodelo_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_balanceado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_balanceado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibsvm_sparse_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_final = SVC(random_state = 0)\n",
    "\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train):\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn = X_train_pol[train_index]\n",
    "    X_testt = X_train_pol[test_index]\n",
    "    y_trainn = y_train.iloc[train_index]\n",
    "    y_testt = y_train.iloc[test_index]\n",
    "\n",
    "    class_counts = y_trainn.value_counts()\n",
    "\n",
    "    oversampling_factors = { #sujetos a cambio\n",
    "        1: 2.2,\n",
    "        2: 1.8,\n",
    "        3: 1.5,\n",
    "        4: 1.3,\n",
    "    }\n",
    "    undersampling_factor = 0.4\n",
    "\n",
    "    X_train_balanceado = []\n",
    "    y_train_balanceado = []\n",
    "\n",
    "    for polarity, count in class_counts.items():\n",
    "      class_data = X_trainn[y_trainn == polarity]\n",
    "      #y_class = y_train[y_train == polarity]\n",
    "\n",
    "      if polarity == 5:\n",
    "        target_size = int(count * undersampling_factor)\n",
    "        #class_data  = resample(class_data, replace=False, n_samples=target_size, random_state=0)\n",
    "        indices = resample(np.arange(X_trainn.shape[0]), replace=False, n_samples=target_size, random_state=0)\n",
    "      else:\n",
    "        factor = oversampling_factors.get(polarity, 1.0)\n",
    "        target_size = int(count * factor)\n",
    "        indices = resample(np.arange(X_trainn.shape[0]), replace=True, n_samples=target_size, random_state=0)\n",
    "        #class_data = resample(class_data, replace=True, n_samples=target_size, random_state=0)\n",
    "\n",
    "      X_balanceado_parcial = X_trainn[indices]\n",
    "      y_balanceado_parcial = y_trainn.iloc[indices]\n",
    "      X_train_balanceado.append(X_balanceado_parcial)\n",
    "      y_train_balanceado.append(y_balanceado_parcial)\n",
    "      #aqui acaba el for, que las identaciones se pusieron de comendiates\n",
    "\n",
    "    X_train_balanceado = vstack(X_train_balanceado)\n",
    "    y_train_balanceado = np.hstack(y_train_balanceado)\n",
    "    modelo_final.fit(X_train_balanceado, y_train_balanceado)\n",
    "\n",
    "    y_pred = modelo_final.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "\n",
    "    #una vez termina todo lo de aquí, a limpiar\n",
    "    del X_trainn, X_testt, y_trainn, y_testt, X_train_balanceado\n",
    "    gc.collect()\n",
    "\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP14XhKhcnX-"
   },
   "source": [
    "Usando polaridad de texto y undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuIahwt8cnX-",
    "outputId": "3f4a5794-2f17-4e52-8f68-4b26da52b949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio del f1-score de 'macro avg' en todas las iteraciones: 0.41603843745733526\n"
     ]
    }
   ],
   "source": [
    "pliegues = 5\n",
    "pliegues_estratificados = StratifiedKFold(n_splits=pliegues, shuffle=True, random_state=0)\n",
    "results = []\n",
    "modelo_prueba = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in pliegues_estratificados.split(X_train_pol, y_train_copy):\n",
    "\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    X_trainn, X_testt = X_train_pol[train_index], X_train_pol[test_index]\n",
    "    y_trainn, y_testt = y_train_copy.iloc[train_index], y_train_copy.iloc[test_index]\n",
    "\n",
    "    # Aplicar under-sampling y over-sampling en los datos de entrenamiento\n",
    "    under_sampler = RandomUnderSampler(random_state=0)\n",
    "    over_sampling = RandomOverSampler(random_state=0)\n",
    "\n",
    "    # Paso 1: Under-sampling\n",
    "    X_resampled, y_resampled = under_sampler.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Paso 2: Over-sampling\n",
    "    #X_resampled, y_resampled = over_sampling.fit_resample(X_trainn, y_trainn)\n",
    "\n",
    "    # Entrenar el modelo con los datos balanceados\n",
    "    modelo_prueba.fit(X_resampled, y_resampled)\n",
    "\n",
    "    y_pred = modelo_prueba.predict(X_testt)\n",
    "    report = classification_report(y_testt, y_pred, output_dict=True)\n",
    "    results.append(report['macro avg']['f1-score'])\n",
    "average_macro_f1 = sum(results) / len(results)\n",
    "print(\"Promedio del f1-score de 'macro avg' en todas las iteraciones:\", average_macro_f1)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_KRhWCs-GY1"
   },
   "source": [
    "**Crear pipeline, junto con los grid_search_view para los clasificadores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "H2gA3Lbv-OPT",
    "outputId": "c02e0d7c-4ca1-40d5-f336-880c0190c167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la maquina de soporte vectorial\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 103, in _analyze\n    doc = decoder(doc)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 236, in decode\n    raise ValueError(\nValueError: np.nan is an invalid document, expected byte or unicode string.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-40ff2f0d61e3>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resultados del perceptrón multicapa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Entrenar el modelo con GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 103, in _analyze\n    doc = decoder(doc)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 236, in decode\n    raise ValueError(\nValueError: np.nan is an invalid document, expected byte or unicode string.\n"
     ]
    }
   ],
   "source": [
    "clasificadores = [SVC(random_state=0), MLPClassifier(max_iter=1000, random_state=0)]\n",
    "param_grid_svc = {\n",
    "                'classifier__C': [0.1, 1, 10],  # Hiperparámetro C para SVM\n",
    "                'classifier__kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n",
    "                'classifier__gamma': ['scale', 'auto']  # Parámetro gamma\n",
    "            }\n",
    "\n",
    "param_grid_mlp = {\n",
    "                'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "                'classifier__activation': ['tanh', 'relu'], #funcion de activacion\n",
    "                'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "            }\n",
    "def probarClasificador(clasificador, parametros, X_train, y_train, X_test, y_test, lock):\n",
    "\n",
    "\n",
    "\n",
    "  pipe = Pipeline([('text_representation', TfidfVectorizer(token_pattern= r'(?u)\\w+|\\w+\\n|\\.|\\¿|\\?', ngram_range=(1,1))), ('classifier',clasificador)])\n",
    "  #aqui, cv hace cross validation por su cuenta, y busca ajustar los mejores hipermarametros a partir del f1-macro\n",
    "  grid_search = GridSearchCV(pipe, parametros, cv=5,scoring='f1_macro')\n",
    "  if isinstance(clasificador, SVC):\n",
    "    print(\"Resultados de la maquina de soporte vectorial\")\n",
    "  else:\n",
    "    print(\"Resultados del perceptrón multicapa\")\n",
    "  # Entrenar el modelo con GridSearchCV\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  y_pred = grid_search.predict(X_test)\n",
    "  with lock:\n",
    "    print(str(grid_search.best_params_))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "hilo_svc = threading.Thread(name=\"Experimento Maquina de soporte vectorial\",target=probarClasificador, args=(clasificadores[0], param_grid_svc, X_train, y_train, X_test, y_test, lock))\n",
    "hilo_mlp = threading.Thread(name=\"Experimento Perceptron multicapa\",target=probarClasificador, args=(clasificadores[1], param_grid_mlp, X_train_copy, y_train_copy, X_test_copy, y_test_copy, lock))\n",
    "#ejecutar hilos\n",
    "hilo_svc.start()\n",
    "hilo_mlp.start()\n",
    "#esperar a que terminen\n",
    "hilo_svc.join()\n",
    "hilo_mlp.join()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
